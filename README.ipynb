{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 详解DNN下的tensorflow优化模块设计\n",
    "---\n",
    "项目将结合***Tensorflow***的特点,详述优化模块设计的关键难点."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "---\n",
    "* Tensorflow  $\\ \\ge\\ $1.3\n",
    "* Python $\\ \\ge\\ $3.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea\n",
    "---\n",
    "### What is ***vec***?\n",
    "一个按照定义顺序出现的张量集合被称为是***vec***.\n",
    "* 例如demo中的网络由2个卷积层(卷积核为$K_1,K_2$)+一个全连接层(权值矩阵为$W$)组成,那么\n",
    "$$[K_1,K_2,W]=tf.trainable\\_variables()$$\n",
    "就是一个***vec***.\n",
    "* 梯度也可以是一个***vec***:\n",
    "$$\\left\\{\\frac{\\partial L}{\\partial K_1}, \\frac{\\partial L}{\\partial K_2}, \\frac{\\partial L}{\\partial W}\\right\\}=tf.gradients(loss, tf.trainable\\_variables())$$\n",
    "\n",
    "### Why ***vec***?\n",
    "将张量集合拉成向量是一件非常破坏并行效率的行为,这也是为什么我们需要重新定义一种新的计算单元***vec***的缘故.\n",
    "### What is ***mat***?\n",
    "具有相同长度,且对应位置张量形状相同的张量集合的集合被称作是***mat***.\n",
    "* 例如demo中cost function对变量的近似\"Hessian矩阵\"就是一个***mat***.\n",
    "$$\n",
    "\\left\\{\\begin{matrix}\n",
    "\\left\\{\\frac{\\partial^2 L}{\\partial K_1 \\partial K_1}, \\frac{\\partial^2 L}{\\partial K_1 \\partial K_2}, \\frac{\\partial^2 L}{\\partial K_1 \\partial W}\\right\\}, \\\\\n",
    "\\left\\{\\frac{\\partial^2 L}{\\partial K_2 \\partial K_1}, \\frac{\\partial^2 L}{\\partial K_2 \\partial K_2}, \\frac{\\partial^2 L}{\\partial K_2 \\partial W}\\right\\}, \\\\\n",
    "\\left\\{\\frac{\\partial^2 L}{\\partial W \\partial K_1}, \\frac{\\partial^2 L}{\\partial W \\partial K_2}, \\frac{\\partial^2 L}{\\partial W \\partial W}\\right\\},\n",
    "\\end{matrix}\\right\\}\n",
    "$$\n",
    "\n",
    "### What is ***eye***?\n",
    "***eye***指的是一类特殊的张量$T_{ijk}^{uvw}=\\delta(i,u)\\delta(j,v)\\delta(k,w)$,如果$T_{ijk}^{uvw}$上下标对应的维度并不相等,则定义$T_{ijk}^{uvw}\\equiv0$.\n",
    "### Why ***eye***?\n",
    "对于***Newton or quasi-Newton's method***而言经常需要用到单位阵,而如果真的在计算中添加单位阵则其维度是非常夸张的,所以采用***eye***的好处就是只关心对角块上的'子单位阵'.降低了运算负担.\n",
    "### What is ***tensor\\_prod***?\n",
    "```python\n",
    "def tensor_prod(x, y):\n",
    "    return tensor\n",
    "```\n",
    "$$U_{ijk} \\otimes V^{uvw} = T_{ijk}^{uvw}$$\n",
    "### What is ***tensor\\_transuvection***?\n",
    "```python\n",
    "def tensor_transuvection(x, y, mode='all'):\n",
    "    if mode == 'all':\n",
    "        return numeric\n",
    "    elif mode == 'right':\n",
    "        return tensor\n",
    "    elif mode == 'auto':\n",
    "        return tensor\n",
    "```\n",
    "$$\n",
    "\\begin{cases}\n",
    "U_{ijk} \\odot V^{ijk} = T, &\\quad \\mathrm{mode=all} \\\\\n",
    "U_{ijk}^{uvw} \\odot V_{uvw} = T_{ijk}, &\\quad \\mathrm{mode=right} \\\\\n",
    "U_{ijk}^{uvw} \\odot V_{uvw}^{rst} = T_{ijk}^{rst}, &\\quad \\mathrm{mode=auto}\n",
    "\\end{cases}\n",
    "$$\n",
    "### What is ***vec_outer_prod***?\n",
    "```python\n",
    "def vec_outer_prod(xs, ys):\n",
    "    return {{tensor, ...}, ...}\n",
    "```\n",
    "$xs = \\{X_1, ...\\}, ys = \\{Y_1, ...\\}$, $X_i, Y_i$ are all tensors\n",
    "$$\n",
    "vec\\_outer\\_prod(xs, ys) = \\{ \\{ X_1 \\otimes Y_1, \\cdots, X_1 \\otimes Y_n \\}, \\cdots, \\{ X_n \\otimes Y_1, \\cdots, X_n \\otimes Y_n \\} \\}\n",
    "$$\n",
    "### What is ***vec_inner_prod***?\n",
    "```python\n",
    "def vec_inner_prod(xs, ys, mode):\n",
    "    if mode == 'all':\n",
    "        return numeric\n",
    "    elif mode == 'right':\n",
    "        return tensor\n",
    "    elif mode == 'auto':\n",
    "        return tensor\n",
    "```\n",
    "$$\n",
    "\\sum\\limits_{i} tensor\\_transuvection(X_i, Y_i, mode), \\quad X_i \\in xs, Y_i \\in ys\n",
    "$$\n",
    "### What is ***mat_vec_prod***?\n",
    "```python\n",
    "def mat_vec_prod(ms, xs, mode):\n",
    "    return {tensor, ...}\n",
    "```\n",
    "$xs = \\{\\{M_{11}, \\cdots, M_{1n}\\}, \\cdots, \\{M_{m1}, \\cdots, M_{mn}\\}\\}, xs = \\{X_1, ..., X_n\\}$, $M_{ij}, X_i$ are all tensors.\n",
    "$$\n",
    "mat\\_vec\\_prod(ms, xs, mode) = \\{vec\\_inner\\_prod(\\{M_{11}, \\cdots, M_{1n}\\}, xs, mode), \\cdots \\}\n",
    "$$\n",
    "### What is ***mat_mat_prod***?\n",
    "```python\n",
    "def mat_mat_prod(mxs, mys, mode):\n",
    "    return {tensor, ...}\n",
    "```\n",
    "Think of mys as column vectors $\\{ ys_1, \\cdots, ys_m \\}$,then use ***mat_vec_prod***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limits\n",
    "---\n",
    "存在以下客观事实:\n",
    "* [1] TensorFlow(包括大部分基于计算图的框架)每一次运行都是强制并行的,除了较少的控制依赖以外.\n",
    "* [2] TensorFlow原生代码应当是不依赖我们设计的优化模块的.\n",
    "* [3] 优化模块应当独立于网络结构的设计流程.\n",
    "* [4] 优化模块应当使得优化算法的实现得到最大限度的并行.\n",
    "* [5] 优化模块应当尽可能避免计算图的扩大.\n",
    "* [6] 优化模块附加产生的计算图不应当存在大量结构重复的子图.\n",
    "* [7] 优化模块尽可能的减少设备间的数据传输,例如除样本以外的张量的设备间传输是不被允许的(即便是样本也应当尽可能避免反复传输同一组样本),但是极少的标量或者bool值的数据传输是被允许的.\n",
    "* [8] 优化模块应当尽可能的减少存储节点(un-trainable Variable)的增加.缩减不必要的计算开销,因为正常情况下,反向传播的浮点数计算开销是会大于正向传播的.这种情况在拟牛顿法中特别严重,如果参数有1e+6个,那么计算开销就会提高1e+6倍.\n",
    "\n",
    "尤其值得注意的一点是,***对于单device而言,线搜索非常的不友好***:\n",
    "```python\n",
    "def step_length(f, g, xk, pk, alpha=1.0, is_newton=False, iters=20):\n",
    "    low = 0.0\n",
    "    high = 1.0\n",
    "\n",
    "    c1 = 1e-4\n",
    "    c2 = 0.9 if is_newton else 0.1\n",
    "\n",
    "    f_0 = f(xk)\n",
    "    g_pk = np.dot(g(xk), pk)\n",
    "\n",
    "    for i in range(iters):\n",
    "        cond1 = f(xk + alpha * pk) <= f_0 + c1 * alpha * g_pk\n",
    "        cond2 = abs(np.dot(g(xk + alpha * pk), pk)) <= c2 * abs(g_pk)\n",
    "        if cond1 and cond2:\n",
    "            return alpha\n",
    "\n",
    "        f_high = f(xk + high * pk)\n",
    "        f_low = f(xk + low * pk)\n",
    "        g_low_pk = np.dot(g(xk + low * pk), pk)\n",
    "\n",
    "        alpha = - g_low_pk * (high**2) / 2 / (f_high - f_low - g_low_pk * high)\n",
    "        if alpha < low or alpha > high:\n",
    "            alpha = (low + high) / 2\n",
    "\n",
    "        g_alpha_pk = np.dot(g(xk + alpha * pk), pk)\n",
    "        if g_alpha_pk > 0:\n",
    "            high = alpha\n",
    "        elif g_alpha_pk <= 0:\n",
    "            low = alpha\n",
    "\n",
    "    return alpha\n",
    "```\n",
    "在并行程序中类似$f\\_high = f(xk + high * pk),\\quad f\\_low = f(xk + low * pk)$这样的计算过程是本质不并行的.只有2种思路可以解决这2个式子的计算:\n",
    "* $x \\leftarrow xk + high * pk $ and output $f(x)$, and then $x \\leftarrow xk + low * pk $ and output $f(x)$\n",
    "* execute assign op $x \\leftarrow xk + high * pk $ in ***Graph/Session 1*** and output $f(x)$ in ***Graph/Session 1***,then execute assign op $x \\leftarrow xk + low * pk $ in ***Graph/Session 2*** and output $f(x)$ in ***Graph/Session 2***.\n",
    "\n",
    "第一种本质串行.第二种虽然本质是并行的,但是计算图的规模至少要扩大一倍,而且计算过程中会中断至少一次进行设备间的数据传输.可见[1~8]想全部实现是不现实的."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}